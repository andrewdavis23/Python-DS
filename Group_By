#### INTERESTING GROUP BY  ###########################################################################################
## Group by the columns of a different dataframe

# Read life_fname into a DataFrame: life
life = pd.read_csv(life_fname, index_col='Country')

#                        1964    1965    1966    1967    1968  ...    2009    2010    2011    2012    2013
# Country                                                      ...                                        
# Afghanistan          33.639  34.152  34.662  35.170  35.674  ...  59.124  59.612  60.079  60.524  60.947
# Albania              65.475  65.863  66.122  66.316  66.500  ...  76.598  76.780  76.979  77.185  77.392
# Algeria              47.953  48.389  48.806  49.205  49.592  ...  70.477  70.615  70.747  70.874  71.000
# Angola               34.604  35.007  35.410  35.816  36.222  ...  50.286  50.689  51.094  51.498  51.899
# Antigua and Barbuda  63.775  64.149  64.511  64.865  65.213  ...  75.263  75.437  75.610  75.783  75.954

# Read regions_fname into a DataFrame: regions
regions = pd.read_csv(regions_fname, index_col='Country')

# region
# Country                                        
# Afghanistan                          South Asia
# Albania                   Europe & Central Asia
# Algeria              Middle East & North Africa
# Angola                       Sub-Saharan Africa
# Antigua and Barbuda                     America

# Group life by regions['region']: life_by_region
life_by_region = life.groupby(regions['region'])

# Print the mean over the '2010' column of life_by_region
print(life_by_region['2010'].mean())

    # region
    # America                       74.037350
    # East Asia & Pacific           73.405750
    # Europe & Central Asia         75.656387
    # Middle East & North Africa    72.805333
    # South Asia                    68.189750
    # Sub-Saharan Africa            57.575080
    
#### DATETIME SERIES GROUP BY DAY OF WEEK ##############################################################################

# Read file: sales
sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)

# Create a groupby object: by_day
by_day = sales.groupby(sales.index.strftime('%a'))

# Create sum: units_sum
units_sum = by_day['Units'].sum()

# Print units_sum
print(units_sum)

    # Mon    48
    # Sat     7
    # Thu    59
    # Tue    13
    # Wed    48
    
#### df.col.transform() ##################################################################################################

# Create a groupby object: by_sex_class
by_sex_class = titanic.groupby(['sex','pclass'])

# Write a function that imputes median
def impute_median(series):
    return series.fillna(series.median())

# Impute age and assign to titanic['age']
titanic.age = by_sex_class.age.transform(impute_median)

# Print the output of titanic.tail(10)
print(titanic.tail(10))


#### Filter by Group without actually grouping the data ######################################################################

# Read the CSV file into a DataFrame: sales
sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)

# Group sales by 'Company': by_company
by_company = sales.groupby('Company')

# Compute the sum of the 'Units' of by_company: by_com_sum
by_com_sum = by_company['Units'].sum()
print(by_com_sum)

    # Company
    # Acme Coporation    34
    # Hooli              30
    # Initech            30
    # Mediacore          45
    # Streeplex          36

# Filter 'Units' where the sum is > 35: by_com_filt
by_com_filt = by_company.filter(lambda g:g['Units'].sum() > 35)
print(by_com_filt)

    #                     Company   Product  Units
    # Date                                           
    # 2015-02-02 21:00:00  Mediacore  Hardware      9
    # 2015-02-04 15:30:00  Streeplex  Software     13
    # 2015-02-09 09:00:00  Streeplex   Service     19
    # 2015-02-09 13:00:00  Mediacore  Software      7
    # 2015-02-19 11:00:00  Mediacore  Hardware     16
    # 2015-02-19 16:00:00  Mediacore   Service     10
    # 2015-02-21 05:00:00  Mediacore  Software      3
    # 2015-02-26 09:00:00  Streeplex   Service      4
    
   
#### Percentage of Titanic survivors under age 10 (mapping and aggreagating a boolean variable ######################################
#  Using maps to make files more readable EX COL:Item in CAO? 

# Create the Boolean Series: under10
under10 = (titanic.age < 10).map({True:'under 10', False:'over 10'})

# Group by under10 and compute the survival rate
survived_mean_1 = titanic.groupby(under10).survived.mean()
print(survived_mean_1)

# Group by under10 and pclass and compute the survival rate
survived_mean_2 = titanic.groupby([under10,'pclass']).survived.mean()
print(survived_mean_2)


# <script.py> output:
#     age
#     over 10     0.366748
#     under 10    0.609756
#     Name: survived, dtype: float64
#     age       pclass
#     over 10   1         0.617555
#               2         0.380392
#               3         0.238897
#     under 10  1         0.750000
#               2         1.000000
#               3         0.446429
    
#### Conditional Groupby:  Number of olympic medals won during the cold war, USA & RUS ####################################################################

# Create a Boolean Series that is True when 'Edition' is between 1952 and 1988: during_cold_war
during_cold_war = (medals.Edition >= 1952) & (medals.Edition <= 1988)

# Extract rows for which 'NOC' is either 'USA' or 'URS': is_usa_urs
is_usa_urs = medals.NOC.isin(['USA','URS'])

# Use during_cold_war and is_usa_urs to create the DataFrame: cold_war_medals
cold_war_medals = medals.loc[(during_cold_war) & (is_usa_urs)]

# Group cold_war_medals by 'NOC'
country_grouped = cold_war_medals.groupby('NOC')

# Create Nsports
Nsports = country_grouped.Sport.nunique().sort_values(ascending=False)

# Print Nsports
print(Nsports)

# NOC
# URS    21
# USA    20


    

