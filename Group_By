#### INTERESTING GROUP BY  ###########################################################################################
## Group by the columns of a different dataframe

# Read life_fname into a DataFrame: life
life = pd.read_csv(life_fname, index_col='Country')

#                        1964    1965    1966    1967    1968  ...    2009    2010    2011    2012    2013
# Country                                                      ...                                        
# Afghanistan          33.639  34.152  34.662  35.170  35.674  ...  59.124  59.612  60.079  60.524  60.947
# Albania              65.475  65.863  66.122  66.316  66.500  ...  76.598  76.780  76.979  77.185  77.392
# Algeria              47.953  48.389  48.806  49.205  49.592  ...  70.477  70.615  70.747  70.874  71.000
# Angola               34.604  35.007  35.410  35.816  36.222  ...  50.286  50.689  51.094  51.498  51.899
# Antigua and Barbuda  63.775  64.149  64.511  64.865  65.213  ...  75.263  75.437  75.610  75.783  75.954

# Read regions_fname into a DataFrame: regions
regions = pd.read_csv(regions_fname, index_col='Country')

# region
# Country                                        
# Afghanistan                          South Asia
# Albania                   Europe & Central Asia
# Algeria              Middle East & North Africa
# Angola                       Sub-Saharan Africa
# Antigua and Barbuda                     America

# Group life by regions['region']: life_by_region
life_by_region = life.groupby(regions['region'])

# Print the mean over the '2010' column of life_by_region
print(life_by_region['2010'].mean())

    # region
    # America                       74.037350
    # East Asia & Pacific           73.405750
    # Europe & Central Asia         75.656387
    # Middle East & North Africa    72.805333
    # South Asia                    68.189750
    # Sub-Saharan Africa            57.575080
    
#### DATETIME SERIES GROUP BY DAY OF WEEK ##############################################################################

# Read file: sales
sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)

# Create a groupby object: by_day
by_day = sales.groupby(sales.index.strftime('%a'))

# Create sum: units_sum
units_sum = by_day['Units'].sum()

# Print units_sum
print(units_sum)

    # Mon    48
    # Sat     7
    # Thu    59
    # Tue    13
    # Wed    48
    
#### df.col.transform() ##############################################################################

# Create a groupby object: by_sex_class
by_sex_class = titanic.groupby(['sex','pclass'])

# Write a function that imputes median
def impute_median(series):
    return series.fillna(series.median())

# Impute age and assign to titanic['age']
titanic.age = by_sex_class.age.transform(impute_median)

# Print the output of titanic.tail(10)
print(titanic.tail(10))


#### Filter by Group without actually grouping the data #############################################

# Read the CSV file into a DataFrame: sales
sales = pd.read_csv('sales.csv', index_col='Date', parse_dates=True)

# Group sales by 'Company': by_company
by_company = sales.groupby('Company')

# Compute the sum of the 'Units' of by_company: by_com_sum
by_com_sum = by_company['Units'].sum()
print(by_com_sum)

    # Company
    # Acme Coporation    34
    # Hooli              30
    # Initech            30
    # Mediacore          45
    # Streeplex          36

# Filter 'Units' where the sum is > 35: by_com_filt
by_com_filt = by_company.filter(lambda g:g['Units'].sum() > 35)
print(by_com_filt)

    #                     Company   Product  Units
    # Date                                           
    # 2015-02-02 21:00:00  Mediacore  Hardware      9
    # 2015-02-04 15:30:00  Streeplex  Software     13
    # 2015-02-09 09:00:00  Streeplex   Service     19
    # 2015-02-09 13:00:00  Mediacore  Software      7
    # 2015-02-19 11:00:00  Mediacore  Hardware     16
    # 2015-02-19 16:00:00  Mediacore   Service     10
    # 2015-02-21 05:00:00  Mediacore  Software      3
    # 2015-02-26 09:00:00  Streeplex   Service      4
    
    
    
    
    
    

