## Stratifying #############################################################################################################
# Class: the output category of data
# Stratify:  Make the original dataset class distribution match the class distribution in both the training and test class

# Create a data with all columns except category_desc
volunteer_X = volunteer.drop("category_desc", axis=1)

# Create a category_desc labels dataset
volunteer_y = volunteer[["category_desc"]]

# Use stratified sampling to split up the dataset according to the volunteer_y dataset
X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)

# Print out the category_desc counts on the training y labels
print(y_train["category_desc"].value_counts())


## KNN & Normalization #####################################################################################################

#### X (Proline is highly variable):
#   Proline  Total phenols   Hue  Nonflavanoid phenols
#0     1065           2.80  1.04                  0.28
#1     1050           2.65  1.05                  0.26
#2     1185           2.80  1.03                  0.30
#3     1480           3.85  0.86                  0.24
#4      735           2.80  1.04                  0.39 

# Split the dataset and labels into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X,y)

# Fit the k-nearest neighbors model to the training data
knn.fit(X_train,y_train)

# Score the model on the test data
print(knn.score(X_test,y_test))

# output is between 0.6 and 0.8

#### LOG NORMALIZATION

# Print out the variance of the Proline column
print(wine.var().loc['Proline'])

## 99166.71735542428

# Apply the log normalization function to the Proline column
wine['Proline'] = np.log(wine['Proline'])

# Check the variance of the normalized Proline column
print(wine.var().loc['Proline'])

## 0.17231366191842018

